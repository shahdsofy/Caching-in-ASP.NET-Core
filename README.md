# üöÄ Caching in ASP.NET Core 

This README explains **everything related to caching** in ASP.NET Core with **clear concepts, diagrams, and real code examples**. 

---

## üìå What is Caching?

Caching is the process of **storing frequently accessed data in fast storage (memory)** to avoid repeated expensive operations such as database calls.

### üéØ Why Caching?

* Improve performance ‚ö°
* Reduce database load
* Improve scalability
* Lower response time

---

## üß† Types of Caching in ASP.NET Core

### 1Ô∏è‚É£ In-Memory Cache (IMemoryCache)

* Stored in application RAM
* Fastest caching option
* Per-server (NOT shared)

### 2Ô∏è‚É£ Distributed Cache (Redis)

* Shared across multiple servers
* Network-based
* Slightly slower than memory

### 3Ô∏è‚É£ Hybrid Cache (Memory + Redis)

* Combines speed + scalability
* Best practice for production

---

## ‚ö° IMemoryCache Example

### Program.cs

```csharp
builder.Services.AddMemoryCache();
```

### Usage

```csharp
public class ProductService
{
    private readonly IMemoryCache _cache;

    public ProductService(IMemoryCache cache)
    {
        _cache = cache;
    }

    public async Task<Product> GetProductAsync(int id)
    {
        return await _cache.GetOrCreateAsync($"product_{id}", async entry =>
        {
            entry.AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(5);
            return await GetProductFromDatabase(id);
        });
    }
}
```

‚úî Ultra-fast
‚ùå Not shared between servers

---

## üåê Distributed Cache (Redis)

### Program.cs

```csharp
builder.Services.AddStackExchangeRedisCache(options =>
{
    options.Configuration = "localhost:6379";
    options.InstanceName = "App:";
});
```

### Usage

```csharp
public async Task<Product> GetProductAsync(int id)
{
    var key = $"product_{id}";
    var cached = await _cache.GetStringAsync(key);

    if (cached != null)
        return JsonSerializer.Deserialize<Product>(cached);

    var product = await GetProductFromDatabase(id);

    await _cache.SetStringAsync(
        key,
        JsonSerializer.Serialize(product),
        new DistributedCacheEntryOptions
        {
            AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(10)
        });

    return product;
}
```

‚úî Shared across servers
‚úî Scalable

---

## üîÅ Cache-Aside Pattern (Most Common)

### Flow

1. Check cache
2. If miss ‚Üí read DB
3. Store result in cache
4. Return response

### Update Handling

```csharp
await UpdateProductInDatabase(product);
_cache.Remove($"product_{product.Id}");
```

‚úî Prevents stale data

---

## ‚è±Ô∏è Cache Expiration Strategies

### Absolute Expiration

```csharp
_cache.Set("rates", data, TimeSpan.FromMinutes(10));
```

### Sliding Expiration

```csharp
_cache.Set(
    "session",
    data,
    new MemoryCacheEntryOptions
    {
        SlidingExpiration = TimeSpan.FromMinutes(20)
    });
```

---

## üîí Concurrency & Thread Safety

* IMemoryCache is thread-safe
* Distributed cache handles concurrency internally
* Async code must avoid blocking locks

---

## üí• What is Cache Stampede?

When:

* Cache expires
* Many requests arrive simultaneously
* All hit the database

‚ùå Result: Database overload

---

## üõ°Ô∏è Cache Stampede Protection (Per-Key Async Lock)

### Concept

* Each cache key has its own lock
* Only ONE request fetches from DB
* Others wait asynchronously

---

## üîë SemaphoreSlim & Per-Key Async Locks (Simple Explanation)

> **This section explains these two concepts in very simple words.**

---

## ‚ùì The Problem (Why We Need Them)

Imagine:

* Cache is empty ‚ùå
* 100 requests arrive at the same time
* All requests ask for the SAME data (`product_1`)

‚ùå Without protection:

```
Request 1 ‚Üí Database
Request 2 ‚Üí Database
Request 3 ‚Üí Database
Request 4 ‚Üí Database
...
```

üí• Database overload

---

## üõ°Ô∏è What is Cache Stampede?

**Cache Stampede** happens when:

* Cache expires or is empty
* Many requests try to load the same data
* All hit the database at once

---

## üîí What is SemaphoreSlim?

Think of `SemaphoreSlim` as a **door with rules** üö™

```csharp
new SemaphoreSlim(1, 1);
```

### What does this mean?

* Only **ONE request** can enter
* Other requests **wait in line**
* When the first finishes, the next enters

### Why SemaphoreSlim?

* Async-safe (works with `async/await`)
* Does NOT block threads
* Perfect for ASP.NET Core

---

## üîë What is Per-Key Async Lock?

Per-key async lock means:

> **Each cache key has its own lock**

### Example:

* `product_1` ‚Üí has its own lock
* `product_2` ‚Üí has a different lock

‚úî Requests for different data run in parallel
‚úî Only SAME data is locked

---

## üß† Visual Explanation 

```
Requests for product_1
   ‚Üì
Semaphore (product_1)
   ‚Üì
ONE request enters
   ‚Üì
Database
   ‚Üì
Cache filled
   ‚Üì
All requests read from cache
```

---

## üß© How It Works in Code

### Step 1: Lock Manager

```csharp
public static class CacheLocks
{
    private static readonly ConcurrentDictionary<string, SemaphoreSlim> _locks = new();

    public static SemaphoreSlim Get(string key)
    {
        return _locks.GetOrAdd(key, _ => new SemaphoreSlim(1, 1));
    }
}
```

‚úî One semaphore per cache key
‚úî Thread-safe

---

### Step 2: Using the Lock

```csharp
var semaphore = CacheLocks.Get(redisKey);
await semaphore.WaitAsync();
try
{
    // double check cache
    // load from database ONLY ONCE
}
finally
{
    semaphore.Release();
}
```

---

## üîÅ Why Double Check Cache After Lock?

Because:

* Another request may have already filled the cache
* So database call is no longer needed

‚úî Prevents duplicate DB calls

---


> We prevent cache stampede using per-key async locks implemented with SemaphoreSlim. This ensures only one request fetches data from the database per cache key, while others wait asynchronously until the cache is populated.

---

## ‚ö†Ô∏è Important Notes

* SemaphoreSlim is per-server
* Redis handles cross-server sharing
* Never use `lock` keyword with async code
* Never use a global lock for all cache keys

---

```csharp
new SemaphoreSlim(1,1);
```

* Allows only one request at a time
* Async-safe
* Does NOT block threads



---



---

## üß™ Diagram ‚Äì Cache Stampede Protection

```
Requests
   ‚Üì
Memory Cache ‚ùå
   ‚Üì
Redis ‚ùå
   ‚Üì
Semaphore (per-key)
   ‚Üì
ONE request
   ‚Üì
Database
   ‚Üì
Cache Filled
   ‚Üì
All requests served from cache
```

---

## üî• Redis vs IMemoryCache Comparison

| Feature     | IMemoryCache | Redis      |
| ----------- | ------------ | ---------- |
| Speed       | Fastest      | Fast       |
| Shared      | ‚ùå No         | ‚úÖ Yes      |
| Persistence | ‚ùå No         | ‚úÖ Optional |
| Scalability | Low          | High       |

---

## üß† Hybrid Cache Strategy (Best Practice)

```
Request
 ‚Üì
Memory Cache
 ‚Üì
Redis
 ‚Üì
Database
```

‚úî Fast
‚úî Scalable
‚úî Production-ready

---


> We use hybrid caching with IMemoryCache and Redis, apply cache-aside pattern, handle expiration properly, and prevent cache stampede using per-key async locks with SemaphoreSlim to protect the database under high concurrency.

---

## ‚úÖ Best Practices

* Short memory TTL
* Longer Redis TTL
* Invalidate cache on updates
* Avoid global locks
* Monitor cache hit ratio

---

## üìå Conclusion

Caching is a critical performance optimization technique. A well-designed caching strategy improves scalability, reduces load, and ensures system reliability.

---

